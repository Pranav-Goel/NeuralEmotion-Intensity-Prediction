{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LE_PC_DMTL_EI - A deep multi-task learning neural network that uses parallely connected components - a CNN/LSTM, fully connected layers, and a feature vector derived from linguistic lexicons and pre-trained activations of a network trained for Emoji detection (deepMoji). So, a *L*exicon and *E*moji detection based, *P*arallely *C*onnected *D*eep *M*ulti-*T*ask *L*earning neural network, optimized specifically for the task of *E*motion *I*ntensity detection.\n",
    "\n",
    "### This architecture handles all the four (can be extended to n) emotions together in a single architecture (as opposed to the architecture in ../LE_PC_DNN/LE_PC_DNN_complete.ipynb). The network branches differently to explot the different pairwise correlations between the different emotions, in a way that optimizes performance for our task. Details can be found in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/pranavgoel/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout\n",
    "from keras.layers import LSTM,Bidirectional,GRU,SimpleRNN\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPooling1D, AveragePooling1D\n",
    "from keras.layers import Input, merge, Dropout\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "#tf.python.control_flow_ops = tf\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pre-trained word2vec based train, dev, and test set tweets representations\n",
    "### Please run the corresponding code in ../Supporting_Codes/ to produce these vector representations.\n",
    "#### Note that these will be vectors of the form (n, l, d) where n is the number of tweets in the set, l is the chosen maximum length (zero padded to have same sequence length = 50 for all samples), and d is the dimensionality of the word embedding (400, since we are using the Twitter word2vec model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1_train_anger shape: (941, 50, 400)\n",
      "x1_test_anger shape: (760, 50, 400)\n"
     ]
    }
   ],
   "source": [
    "emotion = 'anger'\n",
    "x1_train_anger = np.load('../../intermediate_files/word2vec_based_concatenated_vectors/train/'\n",
    "                   +emotion+'.npy')\n",
    "x1_dev_anger = np.load('../../intermediate_files/word2vec_based_concatenated_vectors/dev/'\n",
    "                   +emotion+'.npy')\n",
    "\n",
    "'''\n",
    "we combine the train, dev to serve as the training vector. We have already determined the\n",
    "best performing hyperparamter on the dev set, and just need to see results on test set now.\n",
    "'''\n",
    "\n",
    "x1_train_anger = np.concatenate((x1_train_anger,x1_dev_anger),axis=0)\n",
    "\n",
    "x1_test_anger = np.load('../../intermediate_files/word2vec_based_concatenated_vectors/test/'\n",
    "                   +emotion+'.npy')\n",
    "\n",
    "print('x1_train_anger shape:', x1_train_anger.shape)    # (n, 50, 400)\n",
    "print('x1_test_anger shape:', x1_test_anger.shape)      # (n, 50, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1_train_fear shape: (1257, 50, 400)\n",
      "x1_test_fear shape: (995, 50, 400)\n"
     ]
    }
   ],
   "source": [
    "emotion = 'fear'\n",
    "x1_train_fear = np.load('../../intermediate_files/word2vec_based_concatenated_vectors/train/'\n",
    "                   +emotion+'.npy')\n",
    "x1_dev_fear = np.load('../../intermediate_files/word2vec_based_concatenated_vectors/dev/'\n",
    "                   +emotion+'.npy')\n",
    "\n",
    "'''\n",
    "we combine the train, dev to serve as the training vector. We have already determined the\n",
    "best performing hyperparamter on the dev set, and just need to see results on test set now.\n",
    "'''\n",
    "\n",
    "x1_train_fear = np.concatenate((x1_train_fear,x1_dev_fear),axis=0)\n",
    "\n",
    "x1_test_fear = np.load('../../intermediate_files/word2vec_based_concatenated_vectors/test/'\n",
    "                   +emotion+'.npy')\n",
    "\n",
    "print('x1_train_fear shape:', x1_train_fear.shape)    # (n, 50, 400)\n",
    "print('x1_test_fear shape:', x1_test_fear.shape)      # (n, 50, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1_train_joy shape: (902, 50, 400)\n",
      "x1_test_joy shape: (714, 50, 400)\n"
     ]
    }
   ],
   "source": [
    "emotion = 'joy'\n",
    "x1_train_joy = np.load('../../intermediate_files/word2vec_based_concatenated_vectors/train/'\n",
    "                   +emotion+'.npy')\n",
    "x1_dev_joy = np.load('../../intermediate_files/word2vec_based_concatenated_vectors/dev/'\n",
    "                   +emotion+'.npy')\n",
    "\n",
    "'''\n",
    "we combine the train, dev to serve as the training vector. We have already determined the\n",
    "best performing hyperparamter on the dev set, and just need to see results on test set now.\n",
    "'''\n",
    "\n",
    "x1_train_joy = np.concatenate((x1_train_joy,x1_dev_joy),axis=0)\n",
    "\n",
    "x1_test_joy = np.load('../../intermediate_files/word2vec_based_concatenated_vectors/test/'\n",
    "                   +emotion+'.npy')\n",
    "\n",
    "print('x1_train_joy shape:', x1_train_joy.shape)    # (n, 50, 400)\n",
    "print('x1_test_joy shape:', x1_test_joy.shape)      # (n, 50, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1_train_sadness shape: (860, 50, 400)\n",
      "x1_test_sadness shape: (673, 50, 400)\n"
     ]
    }
   ],
   "source": [
    "emotion = 'sadness'\n",
    "x1_train_sadness = np.load('../../intermediate_files/word2vec_based_concatenated_vectors/train/'\n",
    "                   +emotion+'.npy')\n",
    "x1_dev_sadness = np.load('../../intermediate_files/word2vec_based_concatenated_vectors/dev/'\n",
    "                   +emotion+'.npy')\n",
    "\n",
    "'''\n",
    "we combine the train, dev to serve as the training vector. We have already determined the\n",
    "best performing hyperparamter on the dev set, and just need to see results on test set now.\n",
    "'''\n",
    "\n",
    "x1_train_sadness = np.concatenate((x1_train_sadness,x1_dev_sadness),axis=0)\n",
    "\n",
    "x1_test_sadness = np.load('../../intermediate_files/word2vec_based_concatenated_vectors/test/'\n",
    "                   +emotion+'.npy')\n",
    "\n",
    "print('x1_train_sadness shape:', x1_train_sadness.shape)    # (n, 50, 400)\n",
    "print('x1_test_sadness shape:', x1_test_sadness.shape)      # (n, 50, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With reference to the Figures in our paper, the above is the first of the parallely connected components. It is the concatenated word2vec representation which can be fed to a CNN/LSTM. \n",
    "\n",
    "### Below, we form the average embedding (Layer L2b) by simply taking the mean across the words of the sentence (tweet). This can then serve as input to fully connected layers (component 2 in Figure 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2b_train_anger shape: (941, 400)\n",
      "x2b_test_anger shape: (760, 400)\n"
     ]
    }
   ],
   "source": [
    "x2b_train_anger = np.mean(x1_train_anger, axis=1)\n",
    "x2b_test_anger = np.mean(x1_test_anger, axis=1)\n",
    "\n",
    "print('x2b_train_anger shape:', x2b_train_anger.shape)    # (n, 400)\n",
    "print('x2b_test_anger shape:', x2b_test_anger.shape) # (n, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2b_train_fear shape: (1257, 400)\n",
      "x2b_test_fear shape: (995, 400)\n"
     ]
    }
   ],
   "source": [
    "x2b_train_fear = np.mean(x1_train_fear, axis=1)\n",
    "x2b_test_fear = np.mean(x1_test_fear, axis=1)\n",
    "\n",
    "print('x2b_train_fear shape:', x2b_train_fear.shape)    # (n, 400)\n",
    "print('x2b_test_fear shape:', x2b_test_fear.shape) # (n, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2b_train_joy shape: (902, 400)\n",
      "x2b_test_joy shape: (714, 400)\n"
     ]
    }
   ],
   "source": [
    "x2b_train_joy = np.mean(x1_train_joy, axis=1)\n",
    "x2b_test_joy = np.mean(x1_test_joy, axis=1)\n",
    "\n",
    "print('x2b_train_joy shape:', x2b_train_joy.shape)    # (n, 400)\n",
    "print('x2b_test_joy shape:', x2b_test_joy.shape) # (n, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2b_train_sadness shape: (860, 400)\n",
      "x2b_test_sadness shape: (673, 400)\n"
     ]
    }
   ],
   "source": [
    "x2b_train_sadness = np.mean(x1_train_sadness, axis=1)\n",
    "x2b_test_sadness = np.mean(x1_test_sadness, axis=1)\n",
    "\n",
    "print('x2b_train_sadness shape:', x2b_train_sadness.shape)    # (n, 400)\n",
    "print('x2b_test_sadness shape:', x2b_test_sadness.shape) # (n, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We get the gold labels for our train (=train+dev) and test sets. Note that labels here means the annotated emotion intesities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(941,)\n",
      "(760,)\n"
     ]
    }
   ],
   "source": [
    "emotion = 'anger'\n",
    "y_train_anger = np.concatenate((np.load('../../intermediate_files/gold_label_vectors/train/'\n",
    "                                 +emotion+'.npy'),\n",
    "                          np.load('../../intermediate_files/gold_label_vectors/dev/'\n",
    "                                 +emotion+'.npy')),axis=0)\n",
    "\n",
    "y_test_anger = np.load('../../intermediate_files/gold_label_vectors/test/'\n",
    "                                 +emotion+'.npy')\n",
    "print(y_train_anger.shape)    #(n,)\n",
    "print(y_test_anger.shape)     #(n,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1257,)\n",
      "(995,)\n"
     ]
    }
   ],
   "source": [
    "emotion = 'fear'\n",
    "y_train_fear = np.concatenate((np.load('../../intermediate_files/gold_label_vectors/train/'\n",
    "                                 +emotion+'.npy'),\n",
    "                          np.load('../../intermediate_files/gold_label_vectors/dev/'\n",
    "                                 +emotion+'.npy')),axis=0)\n",
    "\n",
    "y_test_fear = np.load('../../intermediate_files/gold_label_vectors/test/'\n",
    "                                 +emotion+'.npy')\n",
    "print(y_train_fear.shape)    #(n,)\n",
    "print(y_test_fear.shape)     #(n,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(902,)\n",
      "(714,)\n"
     ]
    }
   ],
   "source": [
    "emotion = 'joy'\n",
    "y_train_joy = np.concatenate((np.load('../../intermediate_files/gold_label_vectors/train/'\n",
    "                                 +emotion+'.npy'),\n",
    "                          np.load('../../intermediate_files/gold_label_vectors/dev/'\n",
    "                                 +emotion+'.npy')),axis=0)\n",
    "\n",
    "y_test_joy = np.load('../../intermediate_files/gold_label_vectors/test/'\n",
    "                                 +emotion+'.npy')\n",
    "print(y_train_joy.shape)    #(n,)\n",
    "print(y_test_joy.shape)     #(n,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(860,)\n",
      "(673,)\n"
     ]
    }
   ],
   "source": [
    "emotion = 'sadness'\n",
    "y_train_sadness = np.concatenate((np.load('../../intermediate_files/gold_label_vectors/train/'\n",
    "                                 +emotion+'.npy'),\n",
    "                          np.load('../../intermediate_files/gold_label_vectors/dev/'\n",
    "                                 +emotion+'.npy')),axis=0)\n",
    "\n",
    "y_test_sadness = np.load('../../intermediate_files/gold_label_vectors/test/'\n",
    "                                 +emotion+'.npy')\n",
    "print(y_train_sadness.shape)    #(n,)\n",
    "print(y_test_sadness.shape)     #(n,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, for the third of the parallely connected components - The deepmoji based pre-trained cnn activations (2304 dim. vector) (layer L2c) and our lexicon based features (43 dim. vector) (layer L2d). These can be produced by the corresponding subdirectories in the main directory intermediate_files/ and running the corresponding code in ../Supporting_Codes/ after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2c_train_anger shape: (941, 43)\n",
      "x2c_test_anger shape: (760, 43)\n",
      "x2d_train_anger shape: (941, 2304)\n",
      "x2d_test_anger shape: (760, 2304)\n"
     ]
    }
   ],
   "source": [
    "emotion='anger'\n",
    "x2d_train_anger = np.load('../../intermediate_files/deepmoji_vectors/train/'\n",
    "                                  +emotion+'.npy')\n",
    "x2c_train_anger = np.load('../../intermediate_files/lexicon_vectors/train/'\n",
    "                                  +emotion+'.npy')\n",
    "x2d_dev_anger = np.load('../../intermediate_files/deepmoji_vectors/dev/'\n",
    "                                  +emotion+'.npy')\n",
    "x2c_dev_anger =  np.load('../../intermediate_files/lexicon_vectors/dev/'\n",
    "                                  +emotion+'.npy')\n",
    "x2d_train_anger = np.concatenate((x2d_train_anger,x2d_dev_anger),axis=0)\n",
    "x2c_train_anger = np.concatenate((x2c_train_anger,x2c_dev_anger),axis=0)\n",
    "\n",
    "x2d_test_anger = np.load('../../intermediate_files/deepmoji_vectors/test/'\n",
    "                                  +emotion+'.npy')\n",
    "x2c_test_anger = np.load('../../intermediate_files/lexicon_vectors/test/'\n",
    "                                  +emotion+'.npy')\n",
    "print('x2c_train_anger shape:', x2c_train_anger.shape)   #(n1, 43)\n",
    "print('x2c_test_anger shape:', x2c_test_anger.shape)    #(n2, 43)\n",
    "\n",
    "print('x2d_train_anger shape:', x2d_train_anger.shape)   #(n1, 2304)\n",
    "print('x2d_test_anger shape:', x2d_test_anger.shape)    #(n2, 2304)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2c_train_fear shape: (1257, 43)\n",
      "x2c_test_fear shape: (995, 43)\n",
      "x2d_train_fear shape: (1257, 2304)\n",
      "x2d_test_fear shape: (995, 2304)\n"
     ]
    }
   ],
   "source": [
    "emotion='fear'\n",
    "x2d_train_fear = np.load('../../intermediate_files/deepmoji_vectors/train/'\n",
    "                                  +emotion+'.npy')\n",
    "x2c_train_fear = np.load('../../intermediate_files/lexicon_vectors/train/'\n",
    "                                  +emotion+'.npy')\n",
    "x2d_dev_fear = np.load('../../intermediate_files/deepmoji_vectors/dev/'\n",
    "                                  +emotion+'.npy')\n",
    "x2c_dev_fear =  np.load('../../intermediate_files/lexicon_vectors/dev/'\n",
    "                                  +emotion+'.npy')\n",
    "x2d_train_fear = np.concatenate((x2d_train_fear,x2d_dev_fear),axis=0)\n",
    "x2c_train_fear = np.concatenate((x2c_train_fear,x2c_dev_fear),axis=0)\n",
    "\n",
    "x2d_test_fear = np.load('../../intermediate_files/deepmoji_vectors/test/'\n",
    "                                  +emotion+'.npy')\n",
    "x2c_test_fear = np.load('../../intermediate_files/lexicon_vectors/test/'\n",
    "                                  +emotion+'.npy')\n",
    "print('x2c_train_fear shape:', x2c_train_fear.shape)   #(n1, 43)\n",
    "print('x2c_test_fear shape:', x2c_test_fear.shape)    #(n2, 43)\n",
    "\n",
    "print('x2d_train_fear shape:', x2d_train_fear.shape)   #(n1, 2304)\n",
    "print('x2d_test_fear shape:', x2d_test_fear.shape)    #(n2, 2304)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2c_train_joy shape: (902, 43)\n",
      "x2c_test_joy shape: (714, 43)\n",
      "x2d_train_joy shape: (902, 2304)\n",
      "x2d_test_joy shape: (714, 2304)\n"
     ]
    }
   ],
   "source": [
    "emotion='joy'\n",
    "x2d_train_joy = np.load('../../intermediate_files/deepmoji_vectors/train/'\n",
    "                                  +emotion+'.npy')\n",
    "x2c_train_joy = np.load('../../intermediate_files/lexicon_vectors/train/'\n",
    "                                  +emotion+'.npy')\n",
    "x2d_dev_joy = np.load('../../intermediate_files/deepmoji_vectors/dev/'\n",
    "                                  +emotion+'.npy')\n",
    "x2c_dev_joy =  np.load('../../intermediate_files/lexicon_vectors/dev/'\n",
    "                                  +emotion+'.npy')\n",
    "x2d_train_joy = np.concatenate((x2d_train_joy,x2d_dev_joy),axis=0)\n",
    "x2c_train_joy = np.concatenate((x2c_train_joy,x2c_dev_joy),axis=0)\n",
    "\n",
    "x2d_test_joy = np.load('../../intermediate_files/deepmoji_vectors/test/'\n",
    "                                  +emotion+'.npy')\n",
    "x2c_test_joy = np.load('../../intermediate_files/lexicon_vectors/test/'\n",
    "                                  +emotion+'.npy')\n",
    "print('x2c_train_joy shape:', x2c_train_joy.shape)   #(n1, 43)\n",
    "print('x2c_test_joy shape:', x2c_test_joy.shape)    #(n2, 43)\n",
    "\n",
    "print('x2d_train_joy shape:', x2d_train_joy.shape)   #(n1, 2304)\n",
    "print('x2d_test_joy shape:', x2d_test_joy.shape)    #(n2, 2304)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2c_train_sadness shape: (860, 43)\n",
      "x2c_test_sadness shape: (673, 43)\n",
      "x2d_train_sadness shape: (860, 2304)\n",
      "x2d_test_sadness shape: (673, 2304)\n"
     ]
    }
   ],
   "source": [
    "emotion='sadness'\n",
    "x2d_train_sadness = np.load('../../intermediate_files/deepmoji_vectors/train/'\n",
    "                                  +emotion+'.npy')\n",
    "x2c_train_sadness = np.load('../../intermediate_files/lexicon_vectors/train/'\n",
    "                                  +emotion+'.npy')\n",
    "x2d_dev_sadness = np.load('../../intermediate_files/deepmoji_vectors/dev/'\n",
    "                                  +emotion+'.npy')\n",
    "x2c_dev_sadness =  np.load('../../intermediate_files/lexicon_vectors/dev/'\n",
    "                                  +emotion+'.npy')\n",
    "x2d_train_sadness = np.concatenate((x2d_train_sadness,x2d_dev_sadness),axis=0)\n",
    "x2c_train_sadness = np.concatenate((x2c_train_sadness,x2c_dev_sadness),axis=0)\n",
    "\n",
    "x2d_test_sadness = np.load('../../intermediate_files/deepmoji_vectors/test/'\n",
    "                                  +emotion+'.npy')\n",
    "x2c_test_sadness = np.load('../../intermediate_files/lexicon_vectors/test/'\n",
    "                                  +emotion+'.npy')\n",
    "print('x2c_train_sadness shape:', x2c_train_sadness.shape)   #(n1, 43)\n",
    "print('x2c_test_sadness shape:', x2c_test_sadness.shape)    #(n2, 43)\n",
    "\n",
    "print('x2d_train_sadness shape:', x2d_train_sadness.shape)   #(n1, 2304)\n",
    "print('x2d_test_sadness shape:', x2d_test_sadness.shape)    #(n2, 2304)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the hyperparameters (set to our optimal ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyperparams = {'l2a': 250, 'l3a': 128, 'l3b': 256, 'l5_afs': 125, 'l5_j': 125, 'l6_a': 50, 'l6_fs': 75, 'l6_j': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_anger = [x1_train_anger, x2b_train_anger, x2c_train_anger, x2d_train_anger]\n",
    "x_train_fear = [x1_train_fear, x2b_train_fear, x2c_train_fear, x2d_train_fear]\n",
    "x_train_joy = [x1_train_joy, x2b_train_joy, x2c_train_joy, x2d_train_joy]\n",
    "x_train_sadness = [x1_train_sadness, x2b_train_sadness, x2c_train_sadness, x2d_train_sadness]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_anger = [x1_test_anger, x2b_test_anger, x2c_test_anger, x2d_test_anger]\n",
    "x_test_fear = [x1_test_fear, x2b_test_fear, x2c_test_fear, x2d_test_fear]\n",
    "x_test_joy = [x1_test_joy, x2b_test_joy, x2c_test_joy, x2d_test_joy]\n",
    "x_test_sadness = [x1_test_sadness, x2b_test_sadness, x2c_test_sadness, x2d_test_sadness]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/pranavgoel/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1204: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranavgoel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.2)`\n",
      "  \n",
      "/Users/pranavgoel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.2)`\n",
      "  del sys.path[0]\n",
      "/Users/pranavgoel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Users/pranavgoel/anaconda/lib/python3.6/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/pranavgoel/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranavgoel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "/Users/pranavgoel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "/Users/pranavgoel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "/Users/pranavgoel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "/Users/pranavgoel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:49: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "/Users/pranavgoel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:50: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "/Users/pranavgoel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:51: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "/Users/pranavgoel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    }
   ],
   "source": [
    "# To account for variations in results when models like CNN are involved, you could run \n",
    "# the code 7 times and take average of results as we did in LE-PC-DNN code.\n",
    "\n",
    "#component1\n",
    "l1 = Input(shape=(50,400,))\n",
    "l2a = Conv1D(hyperparams['l2a'], 3, activation='relu')(l1)\n",
    "l2a = GlobalMaxPooling1D()(l2a)\n",
    "l2a = Dropout(p=0.2)(l2a)\n",
    "l3a = Dense(hyperparams['l3a'], activation='relu')(l2a)\n",
    "\n",
    "#component2\n",
    "l2b = Input(shape=(400,))\n",
    "l3b = Dropout(p=0.2)(l2b)\n",
    "l3b = Dense(hyperparams['l3b'], activation='relu')(l3b)\n",
    "\n",
    "#component3\n",
    "l2c = Input(shape=(43,))\n",
    "l2d = Input(shape=(2304,))\n",
    "\n",
    "#merge, component4\n",
    "l4 = merge([l3a, l3b, l2c, l2d], mode='concat', concat_axis=-1)\n",
    "\n",
    "#separation of network for different emotions according to the correlation heuristics\n",
    "l5_afs = Dense(hyperparams['l5_afs'], activation='relu')(l4) #anger, fear, sadness together\n",
    "l5_j = Dense(hyperparams['l5_j'], activation='relu')(l4) #joy separated\n",
    "\n",
    "l6_a = Dense(hyperparams['l6_a'], activation='relu')(l5_afs) #anger separated\n",
    "l6_fs = Dense(hyperparams['l6_fs'], activation='relu')(l5_afs) #fear, sadness together\n",
    "l6_j = Dense(hyperparams['l6_j'], activation='relu')(l5_j)\n",
    "\n",
    "output_anger = Dense(1, activation='sigmoid')(l6_a)\n",
    "output_fear = Dense(1, activation='sigmoid')(l6_fs) #fear separated\n",
    "output_joy = Dense(1, activation='sigmoid')(l6_j)\n",
    "output_sadness = Dense(1, activation='sigmoid')(l6_fs) #sadness separated\n",
    "\n",
    "inp = [l1, l2b, l2c, l2d]\n",
    "model_anger = Model(input=inp, output=output_anger)\n",
    "model_anger.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "model_fear = Model(input=inp, output=output_fear)\n",
    "model_fear.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "model_joy = Model(input=inp, output=output_joy)\n",
    "model_joy.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "model_sadness = Model(input=inp, output=output_sadness)\n",
    "model_sadness.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# To time the architecture\n",
    "#start = timeit.default_timer()\n",
    "\n",
    "model_anger.fit(x_train_anger, y_train_anger, nb_epoch=30, batch_size=16, verbose=0)\n",
    "model_fear.fit(x_train_fear, y_train_fear, nb_epoch=30, batch_size=16, verbose=0)\n",
    "model_joy.fit(x_train_joy, y_train_joy, nb_epoch=30, batch_size=16, verbose=0)\n",
    "model_sadness.fit(x_train_sadness, y_train_sadness, nb_epoch=30, batch_size=16, verbose=0)\n",
    "\n",
    "# stop = timeit.default_timer()\n",
    "# print('Time taken to train LE-PC-DMTL: ', stop-start)\n",
    "\n",
    "'''\n",
    "You can check the number of trainable parameters by printing model summary as in comments\n",
    "below.\n",
    "'''\n",
    "# print('Network parameters for anger: ', model_anger.summary())\n",
    "\n",
    "# print('Network parameters for fear: ', model_fear.summary())\n",
    "\n",
    "# print('Network parameters for joy: ', model_joy.summary())\n",
    "\n",
    "# print('Network parameters for sadness: ', model_sadness.summary())\n",
    "\n",
    "y_pred_anger = model_anger.predict(x_test_anger)\n",
    "y_pred_fear = model_fear.predict(x_test_fear)\n",
    "y_pred_joy = model_joy.predict(x_test_joy)\n",
    "y_pred_sadness = model_sadness.predict(x_test_sadness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation for LE_PC_DMTL_EI model on Test set for anger\n",
      "0.703353729851\n"
     ]
    }
   ],
   "source": [
    "pearson_correlation_score_anger = pearsonr(y_pred_anger.reshape((y_pred_anger.shape[0],))\n",
    "                                           , y_test_anger)[0]\n",
    "\n",
    "print('Pearson Correlation for LE_PC_DMTL_EI model on Test set for anger')\n",
    "print(pearson_correlation_score_anger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation for LE_PC_DMTL_EI model on Test set for fear\n",
      "0.739634055462\n"
     ]
    }
   ],
   "source": [
    "pearson_correlation_score_fear = pearsonr(y_pred_fear.reshape((y_pred_fear.shape[0],))\n",
    "                                           , y_test_fear)[0]\n",
    "\n",
    "print('Pearson Correlation for LE_PC_DMTL_EI model on Test set for fear')\n",
    "print(pearson_correlation_score_fear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation for LE_PC_DMTL model on Test set for joy\n",
      "0.771512404874\n"
     ]
    }
   ],
   "source": [
    "pearson_correlation_score_joy = pearsonr(y_pred_joy.reshape((y_pred_joy.shape[0],))\n",
    "                                           , y_test_joy)[0]\n",
    "\n",
    "print('Pearson Correlation for LE_PC_DMTL_EI model on Test set for joy')\n",
    "print(pearson_correlation_score_joy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_correlation_score_sadness = pearsonr(y_pred_sadness.reshape((y_pred_sadness.shape[0],))\n",
    "                                           , y_test_sadness)[0]\n",
    "\n",
    "print('Pearson Correlation for LE_PC_DMTL_EI model on Test set for sadness')\n",
    "print(pearson_correlation_score_sadness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
