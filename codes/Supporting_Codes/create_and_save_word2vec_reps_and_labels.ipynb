{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "from gensim.models import Word2Vec          #gensim version == 1.0.1 -> ensure this\n",
    "import gensim.models\n",
    "import numpy as np\n",
    "import wordsegment         #version - 0.8.0\n",
    "from keras.preprocessing import sequence\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_anger = pd.read_csv('../../data/train/anger.txt',delimiter='\\t',header=None)\n",
    "train_fear = pd.read_csv('../../data/train/fear.txt',delimiter='\\t',header=None)\n",
    "train_joy = pd.read_csv('../../data/train/joy.txt',delimiter='\\t',header=None)\n",
    "train_sad = pd.read_csv('../../data/train/sadness.txt',delimiter='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_anger = pd.read_csv('../../data/dev/anger.txt',delimiter='\\t',header=None)\n",
    "dev_fear = pd.read_csv('../../data/dev/fear.txt',delimiter='\\t',header=None)\n",
    "dev_joy = pd.read_csv('../../data/dev/joy.txt',delimiter='\\t',header=None)\n",
    "dev_sad = pd.read_csv('../../data/dev/sadness.txt',delimiter='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_anger = pd.read_csv('../../data/test/anger.txt',delimiter='\\t',header=None)\n",
    "test_fear = pd.read_csv('../../data/test/fear.txt',delimiter='\\t',header=None)\n",
    "test_joy = pd.read_csv('../../data/test/joy.txt',delimiter='\\t',header=None)\n",
    "test_sad = pd.read_csv('../../data/test/sadness.txt',delimiter='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_anger_tweets = list(train_anger[1])\n",
    "train_anger_intensities = list(train_anger[3])\n",
    "train_fear_tweets = list(train_fear[1])\n",
    "train_fear_intensities = list(train_fear[3])\n",
    "train_sad_tweets = list(train_sad[1])\n",
    "train_sad_intensities = list(train_sad[3])\n",
    "train_joy_tweets = list(train_joy[1])\n",
    "train_joy_intensities = list(train_joy[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_anger_tweets = list(dev_anger[1])\n",
    "dev_anger_intensities = list(dev_anger[3])\n",
    "dev_fear_tweets = list(dev_fear[1])\n",
    "dev_fear_intensities = list(dev_fear[3])\n",
    "dev_sad_tweets = list(dev_sad[1])\n",
    "dev_sad_intensities = list(dev_sad[3])\n",
    "dev_joy_tweets = list(dev_joy[1])\n",
    "dev_joy_intensities = list(dev_joy[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_anger_tweets = list(test_anger[1])\n",
    "test_anger_intensities = list(test_anger[3])\n",
    "test_fear_tweets = list(test_fear[1])\n",
    "test_fear_intensities = list(test_fear[3])\n",
    "test_sad_tweets = list(test_sad[1])\n",
    "test_sad_intensities = list(test_sad[3])\n",
    "test_joy_tweets = list(test_joy[1])\n",
    "test_joy_intensities = list(test_joy[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the pre-trained Twitter word2vec model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../../intermediate_files/wordvec_model/word2vec_twitter_model.bin',binary=True,unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "we set the length to which each tweet vector will be zero padded to.\n",
    "#this is based on the maximum length we got on the training set - we do not want to remove\n",
    "#any words.\n",
    "'''\n",
    "maxlen=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some of the preprocessing steps are inspired from Akhtar et al. - \"IITP at EmoInt-2017:\n",
    "#Measuring Intensity of Emotions using Sentence Embeddings and Optimized Features\"\n",
    "\n",
    "#this function takes a string (the tweet), and gives the processed list of words\n",
    "def tweet_to_wordlist(s):\n",
    "    \n",
    "    #remove characters outside the ascii code 128\n",
    "    s = ''.join([i if ord(i)<128 else ' ' for i in s])\n",
    "\n",
    "    #remove any newline characters\n",
    "    s = s.replace('\\n',' ')\n",
    "    \n",
    "    #tweets mentions user using '@' followed by username. Replace all those with @user\n",
    "    s = re.sub('@[^ ]+','@user',s)\n",
    "    \n",
    "    #remove URLs\n",
    "    s = re.sub(r\"http\\S+\", \"\", s)\n",
    "    \n",
    "    #words can have punctuation at their boundaries - have spaces in between punctuation and words\n",
    "    l = s.split('...')\n",
    "    for i in range(len(l)):\n",
    "        l[i] = l[i].replace('/',' / ').replace('\\\\',' \\ ').replace(',',' , ').replace('.',' . ').replace('?',' ? ').replace('!',' ! ').replace(\"'\",\" ' \").replace(':',' : ').replace(';',' ; ').replace('-',' - ').replace('(',' ( ').replace(')',' ) ').replace('[',' [ ').replace(']',' ] ').replace('&',' & ').replace('*',' * ').replace('{',' { ').replace('}',' } ').replace('-',' - ').replace('`',' ` ').replace('\"',' \" ')\n",
    "    s2 = ' ... '.join(l)\n",
    "    l2 = s2.split(' ')\n",
    "    \n",
    "    #our pre-trained Twitter word2vec model has embeddings of '#' symbols coming together - upto 5 coming together consecutively\n",
    "    for j in range(len(l2)):\n",
    "        x=l2[j].count('#')\n",
    "        y=len(l2[j])\n",
    "        if(x==y and x>5):\n",
    "            l2[j]='#####'\n",
    "        \n",
    "        #wordsegment will convert the hashtag based joined words, for example, it will segment #iamthebest to ['i','am','the','best']\n",
    "        if l2[j] not in model:\n",
    "            l2[j] = ' '.join(wordsegment.segment(l2[j]))\n",
    "            \n",
    "    s3 = ' '.join(l2)\n",
    "     \n",
    "    #convert words with lots of vowels to their normal form, for example, 'loooooveee' to 'love'    \n",
    "    l3 = s3.split()        \n",
    "    for k in range(len(l3)):\n",
    "        if l3[k] not in model:\n",
    "            l3[k] = ''.join(''.join(s)[:2] for _, s in itertools.groupby(l3[k]))\n",
    "            if l3[k] not in model:\n",
    "                l3[k] = ''.join(''.join(s)[:1] for _, s in itertools.groupby(l3[k]))\n",
    "    for e in range(len(l3)):\n",
    "        if l3[e] not in model:\n",
    "            l3[e] = l3[e].lower()\n",
    "    \n",
    "    #'goin' to 'going', etc. - completing the present continuous verbs\n",
    "    for q in range(len(l3)):\n",
    "        if l3[q] not in model:\n",
    "            if len(l3[q])>3:\n",
    "                if l3[q][-3:]!='ing':\n",
    "                    if l3[q][-2:]=='ng' or l3[q][-2:]=='in':\n",
    "                        w2 = l3[q][:-2]+'ing'\n",
    "                        if nltk.pos_tag([w2])[0][1]=='VBG':\n",
    "                            l3[q] = w2\n",
    "    for r in range(len(l3)):\n",
    "        if l3[r] not in model:\n",
    "            l3[r] = l3[r].lower()\n",
    "    \n",
    "    s4=' '.join(l3)\n",
    "    \n",
    "    #remove any extra spaces\n",
    "    s4=re.sub(' +',' ',s4)\n",
    "    \n",
    "    return s4.split()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#concatenate the word2vec embedding of all the words of input list of words\n",
    "def makeFeatureVecConcat(words):\n",
    "    featureVec = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in model: \n",
    "            featureVec.append(model[word])\n",
    "        else:\n",
    "            if '#' in word:\n",
    "                word=word.replace('#','')\n",
    "                if word in model: \n",
    "                    featureVec.append(model[word])\n",
    "\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, we form the concatenated Twitter word2vec based vector representations of the tweets in train, dev and test sets separately for all the emotions\n",
    "### Then we save them to the intermediate_files folder, where they can then be used in the codes for the various neural models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_anger_vecs = []\n",
    "train_fear_vecs = []\n",
    "train_joy_vecs = []\n",
    "train_sadness_vecs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for s in train_anger_tweets:\n",
    "    train_anger_vecs.append(makeFeatureVecConcat(tweet_to_wordlist(s)))\n",
    "assert len(train_anger_vecs)==len(train_anger_intensities)\n",
    "\n",
    "for s in train_fear_tweets:\n",
    "    train_fear_vecs.append(makeFeatureVecConcat(tweet_to_wordlist(s)))\n",
    "assert len(train_fear_vecs)==len(train_fear_intensities)\n",
    "\n",
    "for s in train_sad_tweets:\n",
    "    train_sadness_vecs.append(makeFeatureVecConcat(tweet_to_wordlist(s)))\n",
    "assert len(train_sadness_vecs)==len(train_sad_intensities)\n",
    "\n",
    "for s in train_joy_tweets:\n",
    "    train_joy_vecs.append(makeFeatureVecConcat(tweet_to_wordlist(s)))\n",
    "assert len(train_joy_vecs)==len(train_joy_intensities)\n",
    "\n",
    "print('Done forming Training vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_anger_vecs = sequence.pad_sequences(train_anger_vecs, maxlen=maxlen, dtype='float64')\n",
    "train_fear_vecs = sequence.pad_sequences(train_fear_vecs, maxlen=maxlen, dtype='float64')\n",
    "train_joy_vecs = sequence.pad_sequences(train_joy_vecs, maxlen=maxlen, dtype='float64')\n",
    "train_sadness_vecs = sequence.pad_sequences(train_sadness_vecs, maxlen=maxlen, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_anger_vecs.shape)\n",
    "print(train_fear_vecs.shape)\n",
    "print(train_joy_vecs.shape)\n",
    "print(train_sadness_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../../intermediate_files/word2vec_based_concatenated_vectors/train/anger.npy',\n",
    "        train_anger_vecs)\n",
    "np.save('../../intermediate_files/word2vec_based_concatenated_vectors/train/fear.npy',\n",
    "        train_fear_vecs)\n",
    "np.save('../../intermediate_files/word2vec_based_concatenated_vectors/train/joy.npy',\n",
    "        train_joy_vecs)\n",
    "np.save('../../intermediate_files/word2vec_based_concatenated_vectors/train/sadness.npy',\n",
    "        train_sadness_vecs)\n",
    "print('Train vectors saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_anger_vecs = []\n",
    "dev_fear_vecs = []\n",
    "dev_joy_vecs = []\n",
    "dev_sadness_vecs = []\n",
    "\n",
    "for s in dev_anger_tweets:\n",
    "    dev_anger_vecs.append(makeFeatureVecConcat(tweet_to_wordlist(s)))\n",
    "assert len(dev_anger_vecs)==len(dev_anger_intensities)\n",
    "\n",
    "for s in dev_fear_tweets:\n",
    "    dev_fear_vecs.append(makeFeatureVecConcat(tweet_to_wordlist(s)))\n",
    "assert len(dev_fear_vecs)==len(dev_fear_intensities)\n",
    "\n",
    "for s in dev_sad_tweets:\n",
    "    dev_sadness_vecs.append(makeFeatureVecConcat(tweet_to_wordlist(s)))\n",
    "assert len(dev_sadness_vecs)==len(dev_sad_intensities)\n",
    "\n",
    "for s in dev_joy_tweets:\n",
    "    dev_joy_vecs.append(makeFeatureVecConcat(tweet_to_wordlist(s)))\n",
    "assert len(dev_joy_vecs)==len(dev_joy_intensities)\n",
    "\n",
    "print('Done forming Dev vectors')\n",
    "\n",
    "dev_anger_vecs = sequence.pad_sequences(dev_anger_vecs, maxlen=maxlen, dtype='float64')\n",
    "dev_fear_vecs = sequence.pad_sequences(dev_fear_vecs, maxlen=maxlen, dtype='float64')\n",
    "dev_joy_vecs = sequence.pad_sequences(dev_joy_vecs, maxlen=maxlen, dtype='float64')\n",
    "dev_sadness_vecs = sequence.pad_sequences(dev_sadness_vecs, maxlen=maxlen, dtype='float64')\n",
    "\n",
    "print(dev_anger_vecs.shape)\n",
    "print(dev_fear_vecs.shape)\n",
    "print(dev_joy_vecs.shape)\n",
    "print(dev_sadness_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../../intermediate_files/word2vec_based_concatenated_vectors/dev/anger.npy',\n",
    "        dev_anger_vecs)\n",
    "np.save('../../intermediate_files/word2vec_based_concatenated_vectors/dev/fear.npy',\n",
    "        dev_fear_vecs)\n",
    "np.save('../../intermediate_files/word2vec_based_concatenated_vectors/dev/joy.npy',\n",
    "        dev_joy_vecs)\n",
    "np.save('../../intermediate_files/word2vec_based_concatenated_vectors/dev/sadness.npy',\n",
    "        dev_sadness_vecs)\n",
    "print('Dev vectors saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_anger_vecs = []\n",
    "test_fear_vecs = []\n",
    "test_joy_vecs = []\n",
    "test_sadness_vecs = []\n",
    "\n",
    "for s in test_anger_tweets:\n",
    "    test_anger_vecs.append(makeFeatureVecConcat(tweet_to_wordlist(s)))\n",
    "assert len(test_anger_vecs)==len(test_anger_intensities)\n",
    "\n",
    "for s in test_fear_tweets:\n",
    "    test_fear_vecs.append(makeFeatureVecConcat(tweet_to_wordlist(s)))\n",
    "assert len(test_fear_vecs)==len(test_fear_intensities)\n",
    "\n",
    "for s in test_sad_tweets:\n",
    "    test_sadness_vecs.append(makeFeatureVecConcat(tweet_to_wordlist(s)))\n",
    "assert len(test_sadness_vecs)==len(test_sad_intensities)\n",
    "\n",
    "for s in test_joy_tweets:\n",
    "    test_joy_vecs.append(makeFeatureVecConcat(tweet_to_wordlist(s)))\n",
    "assert len(test_joy_vecs)==len(test_joy_intensities)\n",
    "\n",
    "print('Done forming Test vectors')\n",
    "\n",
    "test_anger_vecs = sequence.pad_sequences(test_anger_vecs, maxlen=maxlen, dtype='float64')\n",
    "test_fear_vecs = sequence.pad_sequences(test_fear_vecs, maxlen=maxlen, dtype='float64')\n",
    "test_joy_vecs = sequence.pad_sequences(test_joy_vecs, maxlen=maxlen, dtype='float64')\n",
    "test_sadness_vecs = sequence.pad_sequences(test_sadness_vecs, maxlen=maxlen, dtype='float64')\n",
    "\n",
    "print(test_anger_vecs.shape)\n",
    "print(test_fear_vecs.shape)\n",
    "print(test_joy_vecs.shape)\n",
    "print(test_sadness_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../../intermediate_files/word2vec_based_concatenated_vectors/test/anger.npy',\n",
    "        test_anger_vecs)\n",
    "np.save('../../intermediate_files/word2vec_based_concatenated_vectors/test/fear.npy',\n",
    "        test_fear_vecs)\n",
    "np.save('../../intermediate_files/word2vec_based_concatenated_vectors/test/joy.npy',\n",
    "        test_joy_vecs)\n",
    "np.save('../../intermediate_files/word2vec_based_concatenated_vectors/test/sadness.npy',\n",
    "        test_sadness_vecs)\n",
    "print('Test vectors saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (To save the gold annotated intensities for each set (labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save('../../intermediate_files/gold_label_vectors/train/anger.npy',np.array(train_anger_intensities))\n",
    "# np.save('../../intermediate_files/gold_label_vectors/train/fear.npy',np.array(train_fear_intensities))\n",
    "# np.save('../../intermediate_files/gold_label_vectors/train/joy.npy',np.array(train_joy_intensities))\n",
    "# np.save('../../intermediate_files/gold_label_vectors/train/sadness.npy',np.array(train_sad_intensities))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save('../../intermediate_files/gold_label_vectors/dev/anger.npy',np.array(dev_anger_intensities))\n",
    "# np.save('../../intermediate_files/gold_label_vectors/dev/fear.npy',np.array(dev_fear_intensities))\n",
    "# np.save('../../intermediate_files/gold_label_vectors/dev/joy.npy',np.array(dev_joy_intensities))\n",
    "# np.save('../../intermediate_files/gold_label_vectors/dev/sadness.npy',np.array(dev_sad_intensities))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save('../../intermediate_files/gold_label_vectors/test/anger.npy',np.array(test_anger_intensities))\n",
    "# np.save('../../intermediate_files/gold_label_vectors/test/fear.npy',np.array(test_fear_intensities))\n",
    "# np.save('../../intermediate_files/gold_label_vectors/test/joy.npy',np.array(test_joy_intensities))\n",
    "# np.save('../../intermediate_files/gold_label_vectors/test/sadness.npy',np.array(test_sad_intensities))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
